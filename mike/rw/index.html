<!DOCTYPE html>
<html class="ocks-org do-not-copy">
<meta charset="utf-8">
<title>Better Streams for Node.js</title>
<style>

@import url(../style.css);

</style>

<header>
  <aside>June 12, 2014</aside>
  <a href="../" rel="author">Mike Bostock</a>
</header>

<h1>Better Streams for Node.js</h1>

<p>Streams are an essential paradigm for efficient asynchronous systems. Streams break a large, slow operation into many small, fast, incremental computations. For example, instead of reading a massive file all in one go, you can read a few kilobytes at a time. Why is this so important?

<p><i>Streams save memory</i>: you avoid putting the entire file in memory simultaneously. For example, you can count the number of lines in a file with only a few bytes of memory overhead, rather than proportional to the file size. This is particularly useful when computing aggregate statistics, where the data derived from the input (such as a histogram) is much smaller than the input size.

<p><i>Streams lower latency</i>: you can start processing data immediately, rather than waiting for the file’s end. For example, you can pipe a file from disk to an HTTP response and send the initial chunk of data as soon as it is available, improving response time. Or you can report approximate statistics with initial data and then refine them as more data is read.

<p><i>Streams facilitate short-circuiting</i>: you can abort a long operation before it completes. For example, if a file is malformed, you may be able to detect this without reading the entire file. Some operations, like reading Exif data, may not need the entire file to complete. Avoid unnecessary work!

<p>Naturally, <a href="http://nodejs.org">Node.js</a> has a powerful streams API; however, today’s <a href="http://nodejs.org/api/stream.html">streams2</a> API is harder to use than it needs to be. I will now attempt to articulate the shortcomings of streams2, and then argue for a simpler new streams API, <a href="https://github.com/mbostock/rw">rw</a>.

<h2><a href="#current" name="current">#</a>streams2</h2>

<p>[problems with current api]

<p>streams are complicated! lots of different modes: {flowing, non-flowing}, {paused, unpaused}, {piped, unpiped}, {corked, uncorked}. (<a href="http://nodejs.org/docs/v0.11.2/api/stream.html#stream_writable_cork">cork</a> is part of 0.11.)

<p>streams have lots of events, not all of which are supported by all implementations, and not all of which are documented.
  - readable: readable, data, end, close†, pause*, resume*, error
  - writable: drain, prefinish*, finish, pipe, unpipe, error
  - * undocumented internal events
  - † not supported by all streams

<p>convoluted partly due to migration from “classic” to “streams2” API. (classic <a href="http://blog.nodejs.org/2012/12/20/streams2/">noted as “terrible”</a> by Isaac Schlueter) streams2 didn’t really replace the “stream consumer” API, but extended it, and provided base classes to help developers implement streams correctly

<p>but really, complexity stems from a fundamental design flaw: the streams2 API abstracts control flow, hiding it in provided implementations (stream.Readable, stream.Writable, stream.Transform, etc.). readers automatically refill as fast as possible in “flowing” mode, or when read returns null in “non-flowing mode”; writers automatically drain as fast as possible.

<p>so rather than simplifying the streams control flow that made “classic” streams so hard to implement correctly, streams2 hid the control flow behind shared implementations (albeit well-tested and well-optimized ones). but unfortunately it didn’t address the complexity, and it fails to truly hide the underlying complexity through abstraction.

<p>since you don’t directly specify control flow, the design must provide explicit options to choose between different supported modes. likewise the base classes must provide explicit hooks within the control flow to perform operations with affecting control flow (such as transforming data).

<p>arbitrary distinction between stream “consumers” and “implementors”
  - “consumers” can read a stream, write a stream, or pipe streams
  - but transforming data being piped requires becoming an “implementor”?
  - convenient API for most obvious use cases (e.g., pipe)
  - different, less convenient API for other common use cases (e.g., _transform)
  - on your own for other more difficult use cases!
  - black box + hooks

<p>The streams2 API relies heavily on base classes and inheritance, with a few limited hooks to override functionality. It abstracts the control flow, making common cases work magically, but limiting visibility into streams’ underlying behavior and impeding explicit control.

<h2><a href="#proposed" name="proposed">#</a>rw</h2>

<p>make stream control flow visible and explicit
- use simple callbacks rather than named events
- nothing happens “magically”; each operation is explicitly triggered

<p>favor composition of atomic operations over inheritance plus overrides
  - instead of overriding _transform and _flush, you control the flow
  - reading, writing, piping, transforming, etc. all use the same limited API
  - the API scales gracefully from simple use cases to complex ones,
  - remove arbitrary distinction between stream “consumers” and “implementors”
  - avoids arbitrary complexity cliff and leaky abstraction

<p>simplify the control flow and eliminate undocumented hooks
  - reader: fill, end (optional; reader ends automatically)
  - writer: drain, end

<p>ancillary benefits of explicit control flow
  - don’t need to return a copy of the reader’s internal buffer
  - instead, lifecycle of data is explicitly defined: valid until next fill

<h2>design philosohy</h2>

- the purpose of a stream is to break a long operation into small incremental operations
- a long operation (write 1M yesses) is decomposed into small ones (write 1 yes)
- small operations are synchronous and in-memory
- small operations are grouped into medium-sized batches
- batches are then processed asynchronously (e.g., write to disk)

- to avoid Zalgo, combination of asynchronous and synchronous APIs
  - batch operations are asynchronous
  - small operations are synchronous

- small operations are grouped into batches
  - readers return null when the internal buffer is empty: recommend fill
  - writers return false when the internal buffer is full: recommend drain
  - reader is mandatory: no more data is available until you fill
  - writer is advisory: can continue to buffer, at cost to memory

<h2>comparison with streams2</h2>

- some similarities

- readers are similar to the read stream’s “readable” event and read method
  - however, readers do not automatically refill buffer when read returns null
  - instead, you explicitly tell a reader to refill by calling reader.fill
  - this means the reader doesn’t need to create a copy of the internal buffer
  - and you can determine whether and when you want to refill

- writers are similar to the write stream’s write method and “drain” event
  - however, writers do not automatically drain buffer as soon as possible
  - instead, you explicitly tell a writer to drain by calling writer.drain
  - this means the writer can coalesce many small writes
  - and the writer always copies to a (reusable, typically constant-size) internal buffer
- so you don’t have to worry about ownership of data passed to write
- time to write 10M “yes”: fs in 7.2s, rw in 1.3s

- instead of a separate “error” event, a traditional callback is used
- instead of a separate “end” event, reader.ended is true after reader.fill
- no extra hooks (prefinish, finish, close, pause, resume, pipe, unpipe) are needed

<footer>
  <aside>June 12, 2014</aside>
  <a href="../" rel="author">Mike Bostock</a>
</footer>

<script>

GoogleAnalyticsObject = "ga", ga = function() { ga.q.push(arguments); }, ga.q = [], ga.l = +new Date;
ga("create", "UA-48272912-3", "ocks.org");
ga("send", "pageview");

</script>
<script async src="../highlight.min.js"></script>
<script async src="//www.google-analytics.com/analytics.js"></script>
